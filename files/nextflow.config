/*
 * There are a bunch more aws options and attributes to consider. they can be 
 * found here: https://www.nextflow.io/docs/latest/config.html#scope-aws
 */
aws {
    // these will come from our specific AWS setup.
    accessKey = '<YOUR S3 ACCESS KEY>'
    secretKey = '<YOUR S3 SECRET KEY>'

    // making the assumption this is what we'll use. change as needed
    region = 'us-east-1'    

    // not super sure if we'll want/need this
    profile = '<AWS CONFIG PROFILE>' 

    client {
        /*
         * may need to tweak this base don number of batch jobs. i'm assuming 
         * they communicate over http to the node launching the jobs (this may 
         * be a bad assumption, but if so i'm not super sure what will be making
         * an http connection unless that's how the s3 side works)
         */
        maxConnections = 20

        connectionTimeout = 10000
        
        /*
         * def wanna set this based on what we wish to test. there's an open
         * question in the reqs doc about if we should consider things like 
         * intelligent tiering. if we don't go that route, we can try leaving 
         * this unspecified and that has an intelligent default of `STANDARD`
         */
        uploadStorageClass = 'INTELLIGENT_TIERING'
        
        /*
         * if we enable server side encryption of S3. but then there may be key 
         * management to deal with. `aws:kms` is an option and that seems to 
         * need a key using the `storageKmsKeyId` attribute
         */
        storageEncryption = 'AES256'
        
        /*
         * may need:
         *   - endpoint (AWS S3 API endpoint, e.g. s3-us-west-1.amazonaws.com)
         *   - protocol (HTTP vs HTTPS for connections to AWS. doc doesn't say 
         *     what default is)
         */
    }

    // not entirely sure if this goes in the client scope above or if this is 
    // a spearate scope
    batch {
        cliPath = "FILL ME IN WITH PATH TO THE awscli ON THE AMI"
        
        /*
         * Others to consider for batch. almost surely will need "volumes"
         *   - jobRole (The AWS Job Role ARN that needs to be used to execute 
         *     the Batch Job.)
         *   - logsGroup (The name of the logs group used by Batch Jobs 
         *     (default: /aws/batch, requires 22.09.0-edge or later).)
         *   - volumes (One or more container mounts. Mounts can be specified as 
         *     simple e.g. /some/path or canonical format e.g. 
         *     /host/path:/mount/path[:ro|rw]. Multiple mounts can be specified 
         *     separating them with a comma or using a list object.)
         *   - delayBetweenAttempts (Delay between download attempts from S3 
         *     (default 10 sec).)
         *   - maxParallelTransfers (Max parallel upload/download transfer 
         *     operations per job (default: 4).)
         *   - maxTransferAttempts (Max number of downloads attempts from S3 
         *     (default: 1).)
         *   - maxSpotAttempts (Max number of execution attempts of a job 
         *     interrupted by a EC2 spot reclaim event (default: 5, requires 
         *     22.04.0 or later))
         *   - retryMode (The retry mode configuration setting, to accommodate 
         *     rate-limiting on AWS services (default: standard))
         *   - schedulingPriority (The scheduling priority for all tasks when 
         *     using fair-share scheduling for AWS Batch (default: 0, requires 
         *     23.01.0-edge or later))
         *   - shareIdentifier (The share identifier for all tasks when using 
         *     fair-share scheduling for AWS Batch (requires 22.09.0-edge or 
         *     later))
         */
    }
}
